{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4de30427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import to_date, col, explode\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b2898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\hadoop\"\n",
    "os.environ[\"PATH\"]       = r\"C:\\hadoop\\bin;\" + os.environ[\"PATH\"]\n",
    "os.environ[\"AWS_PROFILE\"] = \"ryanj\"\n",
    "os.environ[\"AWS_REGION\"]  = \"us-west-2\"\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\hadoop\"\n",
    "os.environ[\"PATH\"] = rf'{os.environ[\"HADOOP_HOME\"]}\\bin;' + os.environ[\"PATH\"]\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    \"--packages org.apache.hadoop:hadoop-aws:3.3.6 \"\n",
    "    \"--conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \"\n",
    "    \"--conf spark.hadoop.fs.s3a.aws.region=us-west-2 \"\n",
    "    \"--conf spark.hadoop.fs.s3a.aws.credentials.provider=com.amazonaws.auth.DefaultAWSCredentialsProviderChain \"\n",
    "    \"--conf spark.hadoop.io.native.lib.available=false \"\n",
    "    \"--conf spark.hadoop.fs.s3a.fast.upload=true \"\n",
    "    \"--conf spark.hadoop.fs.s3a.fast.upload.buffer=bytebuffer \"\n",
    "    \"pyspark-shell\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cca6ff7e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryan Jewik\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\Ryan Jewik\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .appName(\"sumo-s3a\")\n",
    "         # make sure S3A is wired up; you can keep these if they helped earlier\n",
    "         .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "         .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.profile.ProfileCredentialsProvider\")\n",
    "         .config(\"spark.hadoop.fs.s3a.aws.region\", \"us-west-2\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "334da550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3A impl: org.apache.hadoop.fs.s3a.S3AFileSystem\n"
     ]
    }
   ],
   "source": [
    "hc = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "print(\"S3A impl:\", hc.get(\"fs.s3a.impl\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85b39845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a writable local temp dir\n",
    "spark_local = r\"C:\\tmp\\spark\"\n",
    "pathlib.Path(spark_local).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdc35f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark: 4.0.1\n",
      "Hadoop: 3.4.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Spark:\", spark.version)\n",
    "print(\"Hadoop:\", spark.sparkContext._jvm.org.apache.hadoop.util.VersionInfo.getVersion())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29b07e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.hadoop.fs.s3a.connection.establish.timeout\", \"30000\")   # 30s\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.threads.keepalivetime\",        \"60000\")   # 60s\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.connection.ttl\",              \"300000\")   # 5m\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.retry.interval\",                 \"500\")   # 500ms\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.retry.throttle.interval\",        \"100\")   # 100ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea81d742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keepalive: 60000\n"
     ]
    }
   ],
   "source": [
    "# convert human-friendly durations to plain integers (milliseconds)\n",
    "hconf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "\n",
    "fixes = {\n",
    "    \"fs.s3a.connection.establish.timeout\": \"30000\",    # 30s\n",
    "    \"fs.s3a.threads.keepalivetime\": \"60000\",           # 60s\n",
    "    \"fs.s3a.retry.interval\": \"500\",                    # 500ms\n",
    "    \"fs.s3a.retry.throttle.interval\": \"100\",           # 100ms\n",
    "    \"fs.s3a.connection.ttl\": \"300000\",                 # 5m\n",
    "    \"fs.s3a.assumed.role.session.duration\": \"1800000\"  # 30m\n",
    "}\n",
    "for k, v in fixes.items():\n",
    "    hconf.set(k, v)\n",
    "\n",
    "# (optional) double-check one\n",
    "print(\"keepalive:\", hconf.get(\"fs.s3a.threads.keepalivetime\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5136993d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offenders before fix:\n",
      "  fs.s3a.vectored.read.max.merged.size = 2M\n",
      "  yarn.router.subcluster.cleaner.interval.time = 60s\n",
      "  yarn.resourcemanager.delegation.token.remove-scan-interval = 1h\n",
      "  yarn.router.interceptor.user-thread-pool.keep-alive-time = 30s\n",
      "  fs.s3a.multipart.purge.age = 24h\n",
      "  yarn.federation.state-store.heartbeat.initial-delay = 30s\n",
      "  hadoop.security.groups.shell.command.timeout = 0s\n",
      "  hadoop.service.shutdown.timeout = 30s\n",
      "  yarn.federation.state-store.clean-up-retry-sleep-time = 1s\n",
      "  fs.s3a.multipart.threshold = 128M\n",
      "  yarn.router.subcluster.heartbeat.expiration.time = 30m\n",
      "  yarn.resourcemanager.delegation-token-renewer.thread-retry-interval = 60s\n",
      "  fs.s3a.block.size = 32M\n",
      "  yarn.router.submit.interval.time = 10ms\n",
      "  yarn.federation.gpg.webapp.connect-timeout = 30s\n",
      "  yarn.federation.state-store.sql.max-life-time = 30m\n",
      "  fs.s3a.multipart.size = 64M\n",
      "  yarn.federation.state-store.sql.conn-time-out = 10s\n",
      "  fs.azure.sas.expiry.period = 90d\n",
      "  yarn.resourcemanager.delegation-token-renewer.thread-timeout = 60s\n",
      "  yarn.dispatcher.print-thread-pool.keep-alive-time = 10s\n",
      "  yarn.federation.gpg.webapp.read-timeout = 30s\n",
      "  yarn.apps.cache.expire = 30s\n",
      "  yarn.federation.amrmproxy.register.uam.interval = 100ms\n",
      "  fs.s3a.connection.timeout = 200s\n",
      "  yarn.federation.state-store.sql.idle-time-out = 10m\n",
      "  yarn.federation.gpg.policy.generator.interval = 1h\n",
      "  yarn.federation.gpg.subcluster.heartbeat.expiration-ms = 30m\n",
      "\n",
      "Normalized to milliseconds:\n",
      "  fs.s3a.vectored.read.max.merged.size: 2M -> 120000\n",
      "  yarn.router.subcluster.cleaner.interval.time: 60s -> 60000\n",
      "  yarn.resourcemanager.delegation.token.remove-scan-interval: 1h -> 3600000\n",
      "  yarn.router.interceptor.user-thread-pool.keep-alive-time: 30s -> 30000\n",
      "  fs.s3a.multipart.purge.age: 24h -> 86400000\n",
      "  yarn.federation.state-store.heartbeat.initial-delay: 30s -> 30000\n",
      "  hadoop.security.groups.shell.command.timeout: 0s -> 0\n",
      "  hadoop.service.shutdown.timeout: 30s -> 30000\n",
      "  yarn.federation.state-store.clean-up-retry-sleep-time: 1s -> 1000\n",
      "  fs.s3a.multipart.threshold: 128M -> 7680000\n",
      "  yarn.router.subcluster.heartbeat.expiration.time: 30m -> 1800000\n",
      "  yarn.resourcemanager.delegation-token-renewer.thread-retry-interval: 60s -> 60000\n",
      "  fs.s3a.block.size: 32M -> 1920000\n",
      "  yarn.router.submit.interval.time: 10ms -> 10\n",
      "  yarn.federation.gpg.webapp.connect-timeout: 30s -> 30000\n",
      "  yarn.federation.state-store.sql.max-life-time: 30m -> 1800000\n",
      "  fs.s3a.multipart.size: 64M -> 3840000\n",
      "  yarn.federation.state-store.sql.conn-time-out: 10s -> 10000\n",
      "  fs.azure.sas.expiry.period: 90d -> 7776000000\n",
      "  yarn.resourcemanager.delegation-token-renewer.thread-timeout: 60s -> 60000\n",
      "  yarn.dispatcher.print-thread-pool.keep-alive-time: 10s -> 10000\n",
      "  yarn.federation.gpg.webapp.read-timeout: 30s -> 30000\n",
      "  yarn.apps.cache.expire: 30s -> 30000\n",
      "  yarn.federation.amrmproxy.register.uam.interval: 100ms -> 100\n",
      "  fs.s3a.connection.timeout: 200s -> 200000\n",
      "  yarn.federation.state-store.sql.idle-time-out: 10m -> 600000\n",
      "  yarn.federation.gpg.policy.generator.interval: 1h -> 3600000\n",
      "  yarn.federation.gpg.subcluster.heartbeat.expiration-ms: 30m -> 1800000\n",
      "\n",
      "Remaining with units (should be empty): []\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "hconf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "\n",
    "# report any time-looking values with units\n",
    "offenders = []\n",
    "it = hconf.iterator()\n",
    "pairs = []\n",
    "while it.hasNext():\n",
    "    e = it.next()\n",
    "    pairs.append((e.getKey(), e.getValue()))\n",
    "\n",
    "r_time = re.compile(r\"^\\s*\\d+\\s*(ms|s|m|h|d)\\s*$\", re.I)\n",
    "\n",
    "for k, v in pairs:\n",
    "    if r_time.match(str(v)):\n",
    "        offenders.append((k, v))\n",
    "\n",
    "print(\"Offenders before fix:\")\n",
    "for k, v in offenders:\n",
    "    print(f\"  {k} = {v}\")\n",
    "\n",
    "# convert unit-suffixed durations to milliseconds\n",
    "UNIT_MS = {\"ms\":1, \"s\":1000, \"m\":60_000, \"h\":3_600_000, \"d\":86_400_000}\n",
    "\n",
    "def to_ms_str(val):\n",
    "    m = re.fullmatch(r\"\\s*(\\d+)\\s*(ms|s|m|h|d)\\s*\", str(val).lower())\n",
    "    if not m: \n",
    "        return None\n",
    "    n = int(m.group(1)); u = m.group(2)\n",
    "    return str(n * UNIT_MS[u])\n",
    "\n",
    "changed = []\n",
    "for k, v in pairs:\n",
    "    ms = to_ms_str(v)\n",
    "    if ms and ms != v:\n",
    "        hconf.set(k, ms)\n",
    "        changed.append((k, v, ms))\n",
    "\n",
    "print(\"\\nNormalized to milliseconds:\")\n",
    "for k, old, new in changed:\n",
    "    print(f\"  {k}: {old} -> {new}\")\n",
    "\n",
    "# show any stragglers still using units\n",
    "left = []\n",
    "it = hconf.iterator()\n",
    "while it.hasNext():\n",
    "    e = it.next()\n",
    "    if r_time.match(str(e.getValue())):\n",
    "        left.append((e.getKey(), e.getValue()))\n",
    "print(\"\\nRemaining with units (should be empty):\", left)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2d803a",
   "metadata": {},
   "source": [
    "# bronze reads and silver writes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "215e87da",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\", \"true\")  # avoids choking on stray files\n",
    "\n",
    "df = (spark.read\n",
    "      .option(\"multiLine\", \"true\")                # set to \"false\" if you have JSON Lines\n",
    "      .option(\"mode\", \"PERMISSIVE\")               # or \"FAILFAST\"\n",
    "      .option(\"recursiveFileLookup\", \"true\")\n",
    "      .option(\"pathGlobFilter\", \"*.json\")         # only JSON files\n",
    "      .json(\"s3a://ryans-sumo-bucket/sumo-api-calls/rikishi_matches/\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d25aa43d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- limit: long (nullable = true)\n",
      " |-- records: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- bashoId: string (nullable = true)\n",
      " |    |    |-- day: long (nullable = true)\n",
      " |    |    |-- division: string (nullable = true)\n",
      " |    |    |-- eastId: long (nullable = true)\n",
      " |    |    |-- eastRank: string (nullable = true)\n",
      " |    |    |-- eastShikona: string (nullable = true)\n",
      " |    |    |-- kimarite: string (nullable = true)\n",
      " |    |    |-- matchNo: long (nullable = true)\n",
      " |    |    |-- westId: long (nullable = true)\n",
      " |    |    |-- westRank: string (nullable = true)\n",
      " |    |    |-- westShikona: string (nullable = true)\n",
      " |    |    |-- winnerEn: string (nullable = true)\n",
      " |    |    |-- winnerId: long (nullable = true)\n",
      " |    |    |-- winnerJp: string (nullable = true)\n",
      " |-- skip: long (nullable = true)\n",
      " |-- total: long (nullable = true)\n",
      "\n",
      "+-------+---+--------+------+------------------+-----------+-----------+-------+------+------------------+-----------+-----------+--------+--------+\n",
      "|bashoId|day|division|eastId|eastRank          |eastShikona|kimarite   |matchNo|westId|westRank          |westShikona|winnerEn   |winnerId|winnerJp|\n",
      "+-------+---+--------+------+------------------+-----------+-----------+-------+------+------------------+-----------+-----------+--------+--------+\n",
      "|201507 |15 |Makuuchi|43    |Maegashira 1 East |Tochinoshin|yorikiri   |13     |3835  |Maegashira 11 West|Kyokutenho |Tochinoshin|43      |        |\n",
      "|201507 |14 |Makuuchi|3247  |Maegashira 4 East |Aminishiki |yorikiri   |9      |3835  |Maegashira 11 West|Kyokutenho |Aminishiki |3247    |        |\n",
      "|201507 |13 |Makuuchi|3835  |Maegashira 11 West|Kyokutenho |yorikiri   |7      |3222  |Maegashira 6 West |Gagamaru   |Gagamaru   |3222    |        |\n",
      "|201507 |12 |Makuuchi|3399  |Maegashira 9 East |Sadanofuji |yorikiri   |4      |3835  |Maegashira 11 West|Kyokutenho |Sadanofuji |3399    |        |\n",
      "|201507 |11 |Makuuchi|3255  |Maegashira 16 East|Takanoiwa  |yorikiri   |4      |3835  |Maegashira 11 West|Kyokutenho |Takanoiwa  |3255    |        |\n",
      "|201507 |10 |Makuuchi|3352  |Maegashira 10 East|Kitataiki  |yorikiri   |5      |3835  |Maegashira 11 West|Kyokutenho |Kyokutenho |3835    |        |\n",
      "|201507 |9  |Makuuchi|3835  |Maegashira 11 West|Kyokutenho |yorikiri   |8      |3204  |Maegashira 7 West |Toyonoshima|Toyonoshima|3204    |        |\n",
      "|201507 |8  |Makuuchi|60    |Maegashira 13 East|Hidenoumi  |uwatenage  |4      |3835  |Maegashira 11 West|Kyokutenho |Kyokutenho |3835    |        |\n",
      "|201507 |7  |Makuuchi|3272  |Maegashira 15 West|Satoyama   |yorikiri   |5      |3835  |Maegashira 11 West|Kyokutenho |Satoyama   |3272    |        |\n",
      "|201507 |6  |Makuuchi|3221  |Maegashira 15 East|Seiro      |yorikiri   |6      |3835  |Maegashira 11 West|Kyokutenho |Seiro      |3221    |        |\n",
      "|201507 |5  |Makuuchi|3140  |Maegashira 14 East|Toyohibiki |sukuinage  |5      |3835  |Maegashira 11 West|Kyokutenho |Kyokutenho |3835    |        |\n",
      "|201507 |4  |Makuuchi|637   |Maegashira 13 West|Chiyotairyu|yorikiri   |6      |3835  |Maegashira 11 West|Kyokutenho |Chiyotairyu|637     |        |\n",
      "|201507 |3  |Makuuchi|4126  |Maegashira 11 East|Tokitenku  |yorikiri   |6      |3835  |Maegashira 11 West|Kyokutenho |Tokitenku  |4126    |        |\n",
      "|201507 |2  |Makuuchi|3158  |Maegashira 12 West|Kotoyuki   |oshidashi  |5      |3835  |Maegashira 11 West|Kyokutenho |Kotoyuki   |3158    |        |\n",
      "|201507 |1  |Makuuchi|17    |Maegashira 12 East|Endo       |yorikiri   |5      |3835  |Maegashira 11 West|Kyokutenho |Endo       |17      |        |\n",
      "|201505 |15 |Makuuchi|3835  |Maegashira 14 West|Kyokutenho |yorikiri   |4      |17    |Maegashira 9 West |Endo       |Endo       |17      |        |\n",
      "|201505 |14 |Makuuchi|3835  |Maegashira 14 West|Kyokutenho |katasukashi|6      |3399  |Maegashira 7 West |Sadanofuji |Kyokutenho |3835    |        |\n",
      "|201505 |13 |Makuuchi|3222  |Maegashira 6 East |Gagamaru   |katasukashi|8      |3835  |Maegashira 14 West|Kyokutenho |Kyokutenho |3835    |        |\n",
      "|201505 |12 |Makuuchi|3248  |Maegashira 14 East|Yoshikaze  |tsukiotoshi|1      |3835  |Maegashira 14 West|Kyokutenho |Yoshikaze  |3248    |        |\n",
      "|201505 |11 |Makuuchi|3246  |Maegashira 12 East|Arawashi   |yorikiri   |2      |3835  |Maegashira 14 West|Kyokutenho |Kyokutenho |3835    |        |\n",
      "+-------+---+--------+------+------------------+-----------+-----------+-------+------+------------------+-----------+-----------+--------+--------+\n",
      "only showing top 20 rows\n",
      "root\n",
      " |-- bashoId: string (nullable = true)\n",
      " |-- day: long (nullable = true)\n",
      " |-- division: string (nullable = true)\n",
      " |-- eastId: long (nullable = true)\n",
      " |-- eastRank: string (nullable = true)\n",
      " |-- eastShikona: string (nullable = true)\n",
      " |-- kimarite: string (nullable = true)\n",
      " |-- matchNo: long (nullable = true)\n",
      " |-- westId: long (nullable = true)\n",
      " |-- westRank: string (nullable = true)\n",
      " |-- westShikona: string (nullable = true)\n",
      " |-- winnerEn: string (nullable = true)\n",
      " |-- winnerId: long (nullable = true)\n",
      " |-- winnerJp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# After your JSON read\n",
    "df.printSchema()\n",
    "# You'll likely see: root\n",
    "#  |-- 0: array (nullable = true)\n",
    "#  |    |-- element: struct (contains your fields)\n",
    "\n",
    "rows = df.select(explode(col(\"records\")).alias(\"match\"))\n",
    "flat = rows.select(\"match.*\")   # expand struct into columns\n",
    "flat.show(20, truncate=False)\n",
    "flat.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bfe81e",
   "metadata": {},
   "source": [
    "convert to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d7d0086",
   "metadata": {},
   "outputs": [],
   "source": [
    "silver = (flat\n",
    "          .dropDuplicates())          # typical dedupe key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13a8886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "silver = (silver\n",
    "         .withColumn(\"year\", F.col(\"bashoId\").cast(\"string\").substr(1,4))\n",
    "          .withColumn(\"month\", F.col(\"bashoId\").cast(\"string\").substr(5,2))\n",
    "          .withColumn(\"match_date\",\n",
    "                     F.to_date(\n",
    "                         F.concat_ws(\"-\", F.col(\"year\"), F.col(\"month\"), F.lpad(F.col(\"day\"), 2, \"0\")),\n",
    "                             \"yyyy-MM-dd\"\n",
    "                         )\n",
    "                     )\n",
    "          .withColumn(\n",
    "              \"match_id\",\n",
    "              F.concat_ws(\"\", F.date_format(F.col(\"match_date\"), \"yyyyMMdd\"), F.col(\"matchNo\").cast(\"string\")).cast(\"long\")\n",
    "          )\n",
    "          .drop(\"year\", \"month\")\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fc4331d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------+------+------------------+-----------+-----------+-------+------+------------------+-----------+-----------+--------+--------+----------+----------+\n",
      "|bashoId|day|division|eastId|          eastRank|eastShikona|   kimarite|matchNo|westId|          westRank|westShikona|   winnerEn|winnerId|winnerJp|match_date|  match_id|\n",
      "+-------+---+--------+------+------------------+-----------+-----------+-------+------+------------------+-----------+-----------+--------+--------+----------+----------+\n",
      "| 201501|  1|Makuuchi|  3835| Maegashira 7 East| Kyokutenho|   yorikiri|     10|   670| Maegashira 7 West| Chiyootori| Kyokutenho|    3835|        |2015-01-01|2015010110|\n",
      "| 201407| 15|Makuuchi|  3835|Maegashira 12 East| Kyokutenho|   yorikiri|      2|  4126|Maegashira 15 West|  Tokitenku|  Tokitenku|    4126|        |2014-07-15| 201407152|\n",
      "| 201309| 10|Makuuchi|  4177|Maegashira 13 West|    Homasho|   yorikiri|     10|  3835| Maegashira 6 West| Kyokutenho|    Homasho|    4177|        |2013-09-10|2013091010|\n",
      "| 201211|  2|Makuuchi|  3835| Maegashira 6 East| Kyokutenho|   yorikiri|     10|  3222| Maegashira 7 East|   Gagamaru| Kyokutenho|    3835|        |2012-11-02|2012110210|\n",
      "| 201005| 10|Makuuchi|  3352| Maegashira 4 East|  Kitataiki|  oshidashi|     10|  3835| Maegashira 7 West| Kyokutenho|  Kitataiki|    3352|        |2010-05-10|2010051010|\n",
      "| 200907| 10|Makuuchi|  3835|   Komusubi 1 East| Kyokutenho|  uwatenage|     15|  3195| Maegashira 2 East|  Tochiozan| Kyokutenho|    3835|        |2009-07-10|2009071015|\n",
      "| 200709|  1|Makuuchi|  3648|Maegashira 13 East|       Ryuo|   yorikiri|      5|  3835|Maegashira 12 West| Kyokutenho| Kyokutenho|    3835|        |2007-09-01| 200709015|\n",
      "| 200611|  1|Makuuchi|  3835| Maegashira 6 East| Kyokutenho|shitatenage|     11|  3847| Maegashira 5 West|Tochinonada|Tochinonada|    3847|        |2006-11-01|2006110111|\n",
      "| 200609| 14|Makuuchi|  3835| Maegashira 4 West| Kyokutenho|katasukashi|     15|  4126| Maegashira 2 West|  Tokitenku|  Tokitenku|    4126|        |2006-09-14|2006091415|\n",
      "| 200411| 10|Makuuchi|  3851|   Sekiwake 1 East| Wakanosato|   yorikiri|     16|  3835| Maegashira 3 West| Kyokutenho| Wakanosato|    3851|        |2004-11-10|2004111016|\n",
      "+-------+---+--------+------+------------------+-----------+-----------+-------+------+------------------+-----------+-----------+--------+--------+----------+----------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "silver.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e024d01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "io.native.lib.available = false\n",
      "fs.s3a.fast.upload = true\n",
      "fs.s3a.fast.upload.buffer = bytebuffer\n"
     ]
    }
   ],
   "source": [
    "# optional sanity checks:\n",
    "hc = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "print(\"io.native.lib.available =\", hc.get(\"io.native.lib.available\"))\n",
    "print(\"fs.s3a.fast.upload =\", hc.get(\"fs.s3a.fast.upload\"))\n",
    "print(\"fs.s3a.fast.upload.buffer =\", hc.get(\"fs.s3a.fast.upload.buffer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15710f2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(silver.write\n",
    " .mode(\"overwrite\")\n",
    " .option(\"compression\", \"snappy\")\n",
    " .partitionBy(\"bashoId\")                        # or by year/month/day\n",
    " .parquet(\"s3a://ryans-sumo-bucket/silver/rikishi_matches/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dedd669",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "      .option(\"multiLine\", \"true\")                # set to \"false\" if you have JSON Lines\n",
    "      .option(\"mode\", \"PERMISSIVE\")               # or \"FAILFAST\"\n",
    "      .option(\"recursiveFileLookup\", \"true\")\n",
    "      .option(\"pathGlobFilter\", \"*.json\")         # only JSON files\n",
    "      .json(\"s3a://ryans-sumo-bucket/sumo-api-calls/rikishis/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c4c8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = df.select(explode(col(\"records\")).alias(\"rikishi\"))\n",
    "flat = rows.select(\"rikishi.*\")   # expand struct into columns\n",
    "silver = (flat\n",
    "          .dropDuplicates())  \n",
    "(silver.write\n",
    " .mode(\"overwrite\")\n",
    " .option(\"compression\", \"snappy\")\n",
    " .parquet(\"s3a://ryans-sumo-bucket/silver/rikishis/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281be5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "      .option(\"multiLine\", \"true\")                # set to \"false\" if you have JSON Lines\n",
    "      .option(\"mode\", \"PERMISSIVE\")               # or \"FAILFAST\"\n",
    "      .option(\"recursiveFileLookup\", \"true\")\n",
    "      .option(\"pathGlobFilter\", \"*.json\")         # only JSON files\n",
    "      .json(\"s3a://ryans-sumo-bucket/sumo-api-calls/rikishi_stats/\"))\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"rikishi_id\",\n",
    "    F.regexp_extract(F.input_file_name(), r\"rikishi_(\\d+)\\.json\", 1)\n",
    ")\n",
    "\n",
    "rows = df.select(explode(col(\"records\")).alias(\"rikishi_stats\"))\n",
    "flat = rows.select(\"rikishi.*\")   # expand struct into columns\n",
    "\n",
    "\n",
    "\n",
    "silver = (flat\n",
    "          .dropDuplicates())  \n",
    "(silver.write\n",
    " .mode(\"overwrite\")\n",
    " .option(\"compression\", \"snappy\")\n",
    " .parquet(\"s3a://ryans-sumo-bucket/silver/rikishi_stats/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59256836",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "      .option(\"multiLine\", \"true\")                # set to \"false\" if you have JSON Lines\n",
    "      .option(\"mode\", \"PERMISSIVE\")               # or \"FAILFAST\"\n",
    "      .option(\"recursiveFileLookup\", \"true\")\n",
    "      .option(\"pathGlobFilter\", \"*.json\")         # only JSON files\n",
    "      .json(\"s3a://ryans-sumo-bucket/sumo-api-calls/basho/\"))\n",
    "rows = df.select(explode(col(\"records\")).alias(\"basho\"))\n",
    "flat = rows.select(\"basho.*\")   # expand struct into columns\n",
    "silver = (flat\n",
    "          .dropDuplicates())  \n",
    "(silver.write\n",
    " .mode(\"overwrite\")\n",
    " .option(\"compression\", \"snappy\")\n",
    " .partitionBy(\"location\")\n",
    " .parquet(\"s3a://ryans-sumo-bucket/silver/bashos/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91298e9",
   "metadata": {},
   "source": [
    "# silvers have been written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cebd6c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "      .option(\"multiLine\", \"true\")                # set to \"false\" if you have JSON Lines\n",
    "      .option(\"mode\", \"PERMISSIVE\")               # or \"FAILFAST\"\n",
    "      .option(\"recursiveFileLookup\", \"true\")\n",
    "      .option(\"pathGlobFilter\", \"*.parquet\")         # only JSON files\n",
    "      .parquet(\"s3a://ryans-sumo-bucket/silver/rikishi_matches/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7cb03ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+------------------+--------------+-----------+-------+------+------------------+-----------+-----------+--------+--------+----------+----------+-------+\n",
      "|day| division|eastId|          eastRank|   eastShikona|   kimarite|matchNo|westId|          westRank|westShikona|   winnerEn|winnerId|winnerJp|match_date|  match_id|westWin|\n",
      "+---+---------+------+------------------+--------------+-----------+-------+------+------------------+-----------+-----------+--------+--------+----------+----------+-------+\n",
      "|  8| Makuuchi|   794| Maegashira 3 East|         Terao|   yorikiri|     14|  4071|   Sekiwake 1 West|       Kaio|       Kaio|    4071|        |1996-05-08|1996050814|      1|\n",
      "|  4|Makushita|  3827| Makushita 17 East|   Chiyotenzan|shitatenage|     24|  3837| Makushita 15 East| Kitazakura|Chiyotenzan|    3827|        |1996-05-04|1996050424|      0|\n",
      "|  3|    Juryo|  3835|      Juryo 7 West|    Kyokutenho|   yorikiri|     12|  5078|      Juryo 1 West|Tokitsunada| Kyokutenho|    3835|        |1996-05-03|1996050312|      0|\n",
      "|  9| Makuuchi|  3833| Maegashira 2 East|    Akinoshima|  uwatenage|     11|  3857| Maegashira 1 West|  Tosanoumi| Akinoshima|    3833|        |1996-05-09|1996050911|      0|\n",
      "| 13| Sandanme|   644|  Sandanme 44 West|Tatsuyamaguchi|   yorikiri|     19|  5187|  Sandanme 41 West|Chiyonokuni|Chiyonokuni|    5187|        |1996-05-13|1996051319|      1|\n",
      "| 14| Makuuchi|  4071|   Sekiwake 1 West|          Kaio|   yorikiri|     19|  3853|      Ozeki 1 West| Takanonami| Takanonami|    3853|        |1996-05-14|1996051419|      1|\n",
      "| 15|Makushita|  3851| Makushita 30 West|        Kogawa|   yorikiri|     12|  3657| Makushita 25 West|  Takanosho|  Takanosho|    3657|        |1996-05-15|1996051512|      1|\n",
      "|  7| Makuuchi|  5370| Maegashira 5 West|         Kenko|  oshidashi|     11|  3857| Maegashira 1 West|  Tosanoumi|  Tosanoumi|    3857|        |1996-05-07|1996050711|      1|\n",
      "| 11| Makuuchi|  1560|Maegashira 10 East|     Mitoizumi|   yorikiri|      4|  4889|Maegashira 15 East| Shikishima| Shikishima|    4889|        |1996-05-11| 199605114|      1|\n",
      "|  1|Makushita|  3860| Makushita 31 East|    Tokitsuumi|shitatenage|     15|  3851| Makushita 30 West|     Kogawa| Tokitsuumi|    3860|        |1996-05-01|1996050115|      0|\n",
      "+---+---------+------+------------------+--------------+-----------+-------+------+------------------+-----------+-----------+--------+--------+----------+----------+-------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ef38647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "738480"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59398a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = (df\n",
    "      .withColumn(\n",
    "        \"westWin\",\n",
    "        F.when(F.col(\"winnerId\") == F.col(\"westId\"), F.lit(1)).otherwise(F.lit(0))\n",
    "        )\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e48b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rikishis = (spark.read\n",
    "      .option(\"multiLine\", \"true\")                # set to \"false\" if you have JSON Lines\n",
    "      .option(\"mode\", \"PERMISSIVE\")               # or \"FAILFAST\"\n",
    "      .option(\"recursiveFileLookup\", \"true\")\n",
    "      .option(\"pathGlobFilter\", \"*.parquet\")         # only JSON files\n",
    "      .parquet(\"s3a://ryans-sumo-bucket/silver/rikishis/\"))\n",
    "\n",
    "rikishi_stats = (spark.read\n",
    "      .option(\"multiLine\", \"true\")                # set to \"false\" if you have JSON Lines\n",
    "      .option(\"mode\", \"PERMISSIVE\")               # or \"FAILFAST\"\n",
    "      .option(\"recursiveFileLookup\", \"true\")\n",
    "      .option(\"pathGlobFilter\", \"*.parquet\")         # only JSON files\n",
    "      .parquet(\"s3a://ryans-sumo-bucket/silver/rikishi_stats/\"))\n",
    "\n",
    "\n",
    "rikishis = (spark.read\n",
    "      .option(\"multiLine\", \"true\")                # set to \"false\" if you have JSON Lines\n",
    "      .option(\"mode\", \"PERMISSIVE\")               # or \"FAILFAST\"\n",
    "      .option(\"recursiveFileLookup\", \"true\")\n",
    "      .option(\"pathGlobFilter\", \"*.parquet\")         # only JSON files\n",
    "      .parquet(\"s3a://ryans-sumo-bucket/silver/bashos/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6913e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
