# Combined docker-compose that inlines frontend/backend and the Airflow stack.
# This single file avoids relying on shell variables and makes paths self-contained
# (relative to the repository root). Use this on EC2: `docker compose -f docker-compose.full.yml up -d --build`.

## Shared anchors copied from airflow/docker-compose.yml for consistent env/volumes
x-spark-common: &spark-common
  build:
    context: ./airflow
    dockerfile: spark.Dockerfile
  volumes:
    - ./airflow/jobs:/opt/spark/work-dir/jobs
    - spark-jars:/opt/spark/jars
  image: spark-s3:3.5.2
  networks: [airflow]
  environment:
    - SPARK_HOME=/opt/spark
    - JAVA_HOME=/opt/java/openjdk
    - MONGO_URI=${MONGO_URI}
    - MONGO_DB_NAME=${MONGO_DB_NAME}
    - DB_HOST=${DB_HOST}
    - DB_PORT=${DB_PORT}
    - DB_NAME=${DB_NAME}
    - DB_USERNAME=${DB_USERNAME}
    - DB_PASSWORD=${DB_PASSWORD}
    - JDBC_URL=jdbc:postgresql://${DB_HOST}:${DB_PORT}/${DB_NAME}
    - AWS_DEFAULT_REGION=${AWS_REGION}
    - AWS_REGION=${AWS_REGION}
    - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
    - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}

x-airflow-common: &airflow-common
  build:
    context: ./airflow
    dockerfile: Dockerfile
  env_file:
    - ./airflow/.env
  environment: &airflow-env
    AIRFLOW__CORE__LOAD_EXAMPLES: "False"
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://${AIRFLOW_PG_USER}:${AIRFLOW_PG_PASSWORD}@airflow-postgres:5432/${AIRFLOW_PG_DB}"
    AIRFLOW_CONN_POSTGRES_DEFAULT: "postgresql+psycopg2://${DB_USERNAME}:${DB_PASSWORD}@${DB_HOST}:${DB_PORT}/${DB_NAME}"
    MONGO_CONN_ID: "mongo_default"
    AIRFLOW_CONN_AWS_DEFAULT: "aws://@/?region_name=${AWS_REGION}&aws_access_key_id=${AWS_ACCESS_KEY_ID}&aws_secret_key=${AWS_SECRET_ACCESS_KEY}"
    AIRFLOW_CONN_SUMO_DB: "postgresql+psycopg2://${DB_USERNAME}:${DB_PASSWORD}@sumo-db:5432/${DB_NAME}"
    JAVA_HOME: /usr/lib/jvm/java-17-openjdk-amd64
    SPARK_HOME: /opt/spark
    AIRFLOW__CORE__EXECUTOR: "LocalExecutor"
    AIRFLOW__CORE__PARALLELISM: "8"
    AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG: "4"
    AIRFLOW__SCHEDULER__STANDALONE_DAG_PROCESSOR: "True"
    AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: "2"
    AIRFLOW__CORE__TASK_RUNNER: "StandardTaskRunner"
    AIRFLOW__CORE__STORE_SERIALIZED_DAGS: "True"
    AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
  volumes:
    - ./airflow/jobs:/opt/airflow/jobs
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
    - spark-jars:/opt/spark/jars
  networks: [airflow]
  depends_on:
    airflow-postgres:
      condition: service_healthy

services:
  # -------------------- Backend (Gin) --------------------
  gin-backend:
    build:
      context: ./backend
    image: sumo-backend:local
    ports:
      - "8080:8080"
    env_file:
      - ./backend/.env
    restart: unless-stopped
    depends_on:
      sumo-db:
        condition: service_healthy
      sumo-db-restore:
        condition: service_completed_successfully
    environment:
      - DB_HOST=sumo-db
      - DB_PORT=${DB_PORT}
      - DB_USERNAME=${DB_USERNAME}
      - DB_PASSWORD=${DB_PASSWORD}
      - DB_NAME=${DB_NAME}
      # JWT secret for signing tokens in dev. In production set a secure secret via env.
      - JWT_SECRET=ryanhideo
      - REDIS_URL=redis://redis:6379/0
    networks:
      - sumo_net
      - airflow

  # -------------------- Frontend (Next.js production) --------------------
  react-frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        # In production we intentionally do NOT bake internal service addresses
        # into the client bundle. Leave NEXT_PUBLIC_BACKEND_URL empty so the
        # client uses the current origin (window.location) which avoids CORS.
        NEXT_PUBLIC_BACKEND_URL: ""
    image: sumo-frontend:local
    ports:
      # Map host 3000 to the container's Next.js dev server (port 3000).
      # The frontend runs a Next dev server inside the container on port 3000,
      # so map host:container as 3000:3000 so the site is reachable at
      # http://localhost:3000 on the host.
      - "3000:3000"
    restart: unless-stopped
    depends_on:
      - gin-backend
    environment:
      - NODE_ENV=production
      # Backend reachable by Docker service name on the compose network
      - BACKEND_URL=http://gin-backend:8080
      # Do NOT expose the internal backend host to client bundles. Keep NEXT_PUBLIC_BACKEND_URL empty
      # so the browser uses same-origin requests (avoids CORS).
      - NEXT_PUBLIC_BACKEND_URL=
    networks:
      - sumo_net
      - airflow

  # -------------------- Spark (shared x-spark-common) --------------------
  spark-master:
    <<: *spark-common
    depends_on:
      spark-jars-init:
        condition: service_completed_successfully
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master"]
    ports:
      - "9090:8080"
      - "7077:7077"

  spark-worker:
    <<: *spark-common
    depends_on: 
      spark-master:
        condition: service_started
      spark-jars-init:
        condition: service_completed_successfully
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077"]
    environment:
      - SPARK_MODE=worker
      # Increase defaults so a few concurrent Spark executors can be scheduled locally.
      # These are still overrideable via .env or environment variables.
      - SPARK_WORKER_CORES=${SPARK_WORKER_CORES:-2}
      - SPARK_WORKER_MEMORY=${SPARK_WORKER_MEMORY:-2g}
      - SPARK_MASTER_URL=spark://spark-master:7077
      - JAVA_HOME=/opt/java/openjdk

  # -------------------- Postgres for Airflow --------------------
  airflow-postgres:
    image: postgres:14.0
    environment:
      - POSTGRES_USER=${AIRFLOW_PG_USER}
      - POSTGRES_PASSWORD=${AIRFLOW_PG_PASSWORD}
      - POSTGRES_DB=${AIRFLOW_PG_DB}
    networks:
      - airflow
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB"]
      interval: 5s
      timeout: 3s
      retries: 20

  # -------------------- Sumo Postgres (dev) --------------------
  sumo-db:
    image: postgres:16
    restart: unless-stopped
    environment:
      - POSTGRES_USER=${DB_USERNAME}
      - POSTGRES_PASSWORD=${DB_PASSWORD}
      - POSTGRES_DB=${DB_NAME}
    volumes:
      - sumo-db-data:/var/lib/postgresql/data
    networks:
      - airflow
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USERNAME} -d ${DB_NAME} -h localhost"]
      interval: 5s
      timeout: 3s
      retries: 20

  sumo-db-restore:
    image: ${SUMO_RESTORE_IMAGE:-postgres:18}   # Debian-based; has bash & coreutils
    restart: "no"
    env_file:
      - ./airflow/.env
    environment:
      - DB_USERNAME=${DB_USERNAME}
      - DB_PASSWORD=${DB_PASSWORD}
      - DB_NAME=${DB_NAME}
      - DB_HOST=sumo-db
      - DB_PORT=5432
    depends_on:
      sumo-db:
        condition: service_healthy
    volumes:
      - ./airflow/db_init:/db_init:ro
    entrypoint:
      - /bin/sh
      - -lc
      - |
          set -eu
          echo "== /db_init contents =="
          ls -la /db_init || true

          # Ensure script exists
          if [ ! -f /db_init/restore_sumo_db.sh ]; then
            echo "FATAL: /db_init/restore_sumo_db.sh not found. Check your volume path."
            exit 1
          fi

          # Strip UTF-8 BOM if present into a temp file
          if [ "$(head -c3 /db_init/restore_sumo_db.sh | od -An -t x1 | tr -d ' \n')" = "efbbbf" ]; then
            tail -c +4 /db_init/restore_sumo_db.sh > /tmp/restore.tmp
          else
            cp /db_init/restore_sumo_db.sh /tmp/restore.tmp
          fi

          # Convert CRLF -> LF and make it executable
          tr -d '\r' < /tmp/restore.tmp > /tmp/restore.sh
          chmod +x /tmp/restore.sh

          # Run under bash
          exec bash /tmp/restore.sh
    networks:
      - airflow


  # -------------------- One-shot DB init (Airflow init) --------------------
  airflow-init:
    <<: *airflow-common
    depends_on:
      airflow-postgres:
        condition: service_healthy
      spark-jars-init:
          condition: service_completed_successfully
    restart: "no"
    command:
      - bash
      - -lc
      - |
        set -euo pipefail

        echo 'Running Airflow DB initialization...'
        airflow db init

        echo 'Creating initial user (idempotent)...'
        airflow users create \
          --username "$AIRFLOW_USERNAME" \
          --firstname "$AIRFLOW_FIRSTNAME" \
          --lastname "$AIRFLOW_LASTNAME" \
          --role Admin \
          --email "$AIRFLOW_EMAIL" \
          --password "$AIRFLOW_PASSWORD" || true

        echo 'Seeding spark_default connection...'
        cat > /tmp/spark_conn.json <<'JSON'
        {
          "conn_type": "spark",
          "host": "spark://spark-master:7077",
          "extra": {
            "deploy_mode": "cluster",
            "spark_binary": "spark-submit",
            "master": "spark://spark-master:7077"
          }
        }
        JSON

        airflow connections delete spark_default || true
        airflow connections add spark_default --conn-json "$(cat /tmp/spark_conn.json)"

        echo 'Seeding mongo_default connection (conn_type="mongo" + extras.srv to satisfy provider)...'
        airflow connections delete mongo_default || true
        if [ -n "${MONGO_URI:-}" ]; then
          echo "Building connection JSON for mongo_default from MONGO_URI"
          python3 -c 'import os,json; from urllib.parse import urlparse,unquote; uri=os.environ.get("MONGO_URI",""); parsed=urlparse(uri); username=unquote(parsed.username) if parsed.username else None; password=unquote(parsed.password) if parsed.password else None; host=parsed.hostname; schema=(parsed.path.lstrip("/") if parsed.path and parsed.path!="/" else os.environ.get("MONGO_DB_NAME")); conn={"conn_type":"mongo","host":host,"login":username,"password":password,"schema":schema,"extra":{"srv":"true"}}; import sys, json as __json; sys.stdout.write(__json.dumps(conn))' > /tmp/mongo_conn.json
          airflow connections add mongo_default --conn-json "$(cat /tmp/mongo_conn.json)" || true
        else
          echo "MONGO_URI not provided; skipping mongo_default seeding."
        fi

        airflow connections get mongo_default

        echo 'Init completed.'

  spark-jars-init:
    image: spark-s3:3.5.2
    command: >
      bash -lc "
        set -e;
        mkdir -p /opt/spark/jars;
        cp -u /opt/spark/jars-baked/* /opt/spark/jars/ 2>/dev/null || true;
        ls -l /opt/spark/jars;
      "
    volumes:
      - spark-jars:/opt/spark/jars
    networks:
      - airflow
    restart: "no"

  # -------------------- Airflow services --------------------
  # -------------------- Redis (for realtime vote counts) --------------------
  redis:
    image: redis:7
    container_name: sumo_redis
    ports:
      - "6379:6379"
    restart: unless-stopped
    networks:
      - sumo_net
      - airflow
  airflow-webserver:
    <<: *airflow-common
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      spark-jars-init:
        condition: service_completed_successfully
    ports:
      - "8081:8080"
    volumes:
      - ./airflow/jobs:/opt/airflow/jobs
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - spark-jars:/opt/spark/jars
      - /var/run/docker.sock:/var/run/docker.sock
    command: >
      bash -lc 'until airflow db check-migrations; do echo "Waiting for DB migrations..."; sleep 3; done; exec airflow webserver'

  airflow-scheduler:
    <<: *airflow-common
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      spark-jars-init:
        condition: service_completed_successfully
    environment:
      <<: *airflow-env
    volumes:
      - ./airflow/jobs:/opt/airflow/jobs
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - spark-jars:/opt/spark/jars
      - /var/run/docker.sock:/var/run/docker.sock
    command: >
      bash -lc 'set -euo pipefail; until airflow db check-migrations; do echo "Waiting for DB migrations..."; sleep 2; done; exec airflow scheduler'

  airflow-dag-processor:
    <<: *airflow-common
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      spark-jars-init:
        condition: service_completed_successfully
    volumes:
      - ./airflow/jobs:/opt/airflow/jobs
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - spark-jars:/opt/spark/jars
      - /var/run/docker.sock:/var/run/docker.sock
    command: >
      bash -lc 'until airflow db check-migrations; do echo "Waiting for DB migrations..."; sleep 3; done; exec airflow dag-processor'

  # -------------------- TLS Reverse Proxy (nginx) --------------------
  proxy:
    build:
      context: ./deploy/nginx
      dockerfile: Dockerfile
    image: sumo-proxy:local
    ports:
      - "80:80"
      - "443:443"
    restart: unless-stopped
    depends_on:
      - react-frontend
      - gin-backend
      - airflow-webserver
    volumes:
      # Mount your Cloudflare origin cert/key here on the EC2 host at /etc/ssl/sumopedia/
      - /etc/ssl/sumopedia/origin-cert.pem:/etc/ssl/sumopedia/origin-cert.pem:ro
      - /etc/ssl/sumopedia/origin-key.pem:/etc/ssl/sumopedia/origin-key.pem:ro
    networks:
      - sumo_net
      - airflow

volumes:
  sumo-db-data: {}
  spark-jars: {}

networks:
  sumo_net:
    driver: bridge
  airflow:
    driver: bridge
