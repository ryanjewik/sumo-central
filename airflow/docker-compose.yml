# docker-compose.yml

x-spark-common: &spark-common
  image: apache/spark:3.5.2-java17-python3
  volumes:
    - ./jobs:/opt/spark/work-dir/jobs
  networks: [airflow]
  environment:
    - SPARK_HOME=/opt/spark
    - JAVA_HOME=/opt/java/openjdk

x-airflow-common: &airflow-common
  build:
    context: .
    dockerfile: Dockerfile
  env_file:
    - .env
  environment: &airflow-env
    AIRFLOW__CORE__LOAD_EXAMPLES: "False"


    # DB connection from .env
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://${AIRFLOW_PG_USER}:${AIRFLOW_PG_PASSWORD}@airflow-postgres:5432/${AIRFLOW_PG_DB}"
    AIRFLOW_CONN_POSTGRES_DEFAULT: "postgresql+psycopg2://${DB_USERNAME}:${DB_PASSWORD}@${DB_HOST}:${DB_PORT}/${DB_NAME}"
    AIRFLOW_CONN_MONGO_DEFAULT: "${MONGO_URI}"
    AIRFLOW_CONN_AWS_DEFAULT: "aws://@/?region_name=${AWS_REGION}&aws_access_key_id=${AWS_ACCESS_KEY_ID}&aws_secret_key=${AWS_SECRET_ACCESS_KEY}"
    # Connection to your Sumo Postgres database (dev service added below)
    AIRFLOW_CONN_SUMO_DB: "postgresql+psycopg2://${DB_USERNAME}:${DB_PASSWORD}@sumo-db:5432/${DB_NAME}"

    JAVA_HOME: /usr/lib/jvm/java-17-openjdk-amd64
    SPARK_HOME: /opt/spark

    # Executor & concurrency
    AIRFLOW__CORE__EXECUTOR: "LocalExecutor"
    AIRFLOW__CORE__PARALLELISM: "1"
    AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG: "1"
    AIRFLOW__SCHEDULER__STANDALONE_DAG_PROCESSOR: "True"
    AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: "1"
    AIRFLOW__CORE__TASK_RUNNER: "StandardTaskRunner"
    AIRFLOW__CORE__STORE_SERIALIZED_DAGS: "True"
    AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
  volumes:
    - ./jobs:/opt/airflow/jobs
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
  networks: [airflow]
  depends_on:
    airflow-postgres:
      condition: service_healthy

services:
  # -------------------- Spark --------------------
  spark-master:
    <<: *spark-common
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master"]
    ports:
      - "9090:8080"
      - "7077:7077"

  spark-worker:
    <<: *spark-common
    depends_on: [spark-master]
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077"]
    environment:
      - SPARK_MODE=worker
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_MASTER_URL=spark://spark-master:7077
      - JAVA_HOME=/opt/java/openjdk

  # -------------------- Postgres --------------------
  airflow-postgres:
    image: postgres:14.0
    environment:
      - POSTGRES_USER=${AIRFLOW_PG_USER}
      - POSTGRES_PASSWORD=${AIRFLOW_PG_PASSWORD}
      - POSTGRES_DB=${AIRFLOW_PG_DB}
    networks: [airflow]
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB"]
      interval: 5s
      timeout: 3s
      retries: 20

  # -------------------- Sumo Postgres (dev) --------------------
  # This Postgres service is intended for local development/testing only.
  # It will initialize from any SQL files placed in ./db_init on first start.
  sumo-db:
    image: postgres:16
    restart: unless-stopped
    environment:
      - POSTGRES_USER=${DB_USERNAME}
      - POSTGRES_PASSWORD=${DB_PASSWORD}
      - POSTGRES_DB=${DB_NAME}
    volumes:
      - sumo-db-data:/var/lib/postgresql/data
    networks: [airflow]
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USERNAME} -d ${DB_NAME} -h localhost"]
      interval: 5s
      timeout: 3s
      retries: 20

  # -------------------- One-shot restore (runs after sumo-db is healthy)
  # Restores any .dump or .sql files found in ./db_init into the sumo-db
  sumo-db-restore:
    image: ${SUMO_RESTORE_IMAGE:-postgres:18}
    restart: "no"
    env_file:
      - .env
    environment:
      - DB_USERNAME=${DB_USERNAME}
      - DB_PASSWORD=${DB_PASSWORD}
      - DB_NAME=${DB_NAME}
      - DB_HOST=sumo-db
      - DB_PORT=5432
      - FORCE_RESTORE=1
    depends_on:
      sumo-db:
        condition: service_healthy
    volumes:
      - ./db_init:/db_init:ro
    entrypoint: ["/bin/bash", "/db_init/restore_sumo_db.sh"]
    networks: [airflow]

  

  # -------------------- One-shot DB init --------------------
  airflow-init:
    <<: *airflow-common
    restart: "no"
    command:
      - bash
      - -lc
      - |
        set -euo pipefail

        echo 'Running Airflow DB initialization...'
        airflow db init

        echo 'Creating initial user (idempotent)...'
        airflow users create \
          --username "$AIRFLOW_USERNAME" \
          --firstname "$AIRFLOW_FIRSTNAME" \
          --lastname "$AIRFLOW_LASTNAME" \
          --role Admin \
          --email "$AIRFLOW_EMAIL" \
          --password "$AIRFLOW_PASSWORD" || true

        echo 'Seeding spark_default connection...'
        cat > /tmp/spark_conn.json <<'JSON'
        {
          "conn_type": "spark",
          "host": "spark://spark-master:7077",
          "extra": {
            "deploy_mode": "cluster",
            "spark_binary": "spark-submit",
            "master": "spark://spark-master:7077"
          }
        }
        JSON

        airflow connections delete spark_default || true
        airflow connections add spark_default --conn-json "$(cat /tmp/spark_conn.json)"

        echo 'Init completed.'


  # -------------------- Webserver --------------------
  airflow-webserver:
    <<: *airflow-common
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    ports:
      - "8081:8080"
    command: >
      bash -lc '
        until airflow db check-migrations; do
          echo "Waiting for DB migrations to finish..."; sleep 3;
        done;
        exec airflow webserver
      '
    volumes:
      - ./jobs:/opt/airflow/jobs
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock

  # -------------------- Scheduler (Execution API INSIDE) --------------------
  airflow-scheduler:
    <<: *airflow-common
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment:
      <<: *airflow-env
    command: >
      bash -lc '
        set -euo pipefail
        until airflow db check-migrations; do
          echo "Waiting for DB migrations to finish..."; sleep 2;
        done;
        exec airflow scheduler
      '
    volumes:
      - ./jobs:/opt/airflow/jobs
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock

  # -------------------- DAG Processor --------------------
  airflow-dag-processor:
    <<: *airflow-common
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    command: >
      bash -lc '
        until airflow db check-migrations; do
          echo "Waiting for DB migrations to finish..."; sleep 3;
        done;
        exec airflow dag-processor
      '
    volumes:
      - ./jobs:/opt/airflow/jobs
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock

volumes:
  sumo-db-data: {}

networks:
  airflow:
