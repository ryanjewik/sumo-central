# docker-compose.yml

x-spark-common: &spark-common
  # Build a Spark image that includes Python 3.8 so workers match the
  # Airflow container's Python minor version (3.8.x). This prevents
  # PySpark driver/executor minor-version mismatch errors.
  build:
    context: .
    dockerfile: spark.Dockerfile
  volumes:
    - ./jobs:/opt/spark/work-dir/jobs
  networks: [airflow]
  environment:
    - SPARK_HOME=/opt/spark
    - JAVA_HOME=/opt/java/openjdk
    # Make Mongo connection info available to Spark containers so apps
    # (or SparkSession.builder configs) can read MONGO_URI/MONGO_DB_NAME.
    - MONGO_URI=${MONGO_URI}
    - MONGO_DB_NAME=${MONGO_DB_NAME}
    # Make Postgres connection info available to Spark containers so
    # Spark jobs can construct JDBC URLs from environment variables.
    - DB_HOST=${DB_HOST}
    - DB_PORT=${DB_PORT}
    - DB_NAME=${DB_NAME}
    - DB_USERNAME=${DB_USERNAME}
    - DB_PASSWORD=${DB_PASSWORD}
    - JDBC_URL=jdbc:postgresql://${DB_HOST}:${DB_PORT}/${DB_NAME}
    # AWS: region and credentials (pulled from .env or host env). Set AWS_DEFAULT_REGION here
    # to ensure Spark driver/executors sign requests with the correct region.
    - AWS_DEFAULT_REGION=${AWS_REGION}
    - AWS_REGION=${AWS_REGION}
    - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
    - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}

x-airflow-common: &airflow-common
  build:
    context: .
    dockerfile: Dockerfile
  env_file:
    - .env
  environment: &airflow-env
    AIRFLOW__CORE__LOAD_EXAMPLES: "False"


    # DB connection from .env
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://${AIRFLOW_PG_USER}:${AIRFLOW_PG_PASSWORD}@airflow-postgres:5432/${AIRFLOW_PG_DB}"
    AIRFLOW_CONN_POSTGRES_DEFAULT: "postgresql+psycopg2://${DB_USERNAME}:${DB_PASSWORD}@${DB_HOST}:${DB_PORT}/${DB_NAME}"
    MONGO_CONN_ID: "mongo_default"
    AIRFLOW_CONN_AWS_DEFAULT: "aws://@/?region_name=${AWS_REGION}&aws_access_key_id=${AWS_ACCESS_KEY_ID}&aws_secret_key=${AWS_SECRET_ACCESS_KEY}"
    # Connection to your Sumo Postgres database (dev service added below)
    AIRFLOW_CONN_SUMO_DB: "postgresql+psycopg2://${DB_USERNAME}:${DB_PASSWORD}@sumo-db:5432/${DB_NAME}"

    JAVA_HOME: /usr/lib/jvm/java-17-openjdk-amd64
    SPARK_HOME: /opt/spark

    # Executor & concurrency
    AIRFLOW__CORE__EXECUTOR: "LocalExecutor"
    # Global parallelism: total concurrent task instances across the scheduler
    AIRFLOW__CORE__PARALLELISM: "16"
    # Max concurrent running task instances per DAG
    AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG: "8"
    AIRFLOW__SCHEDULER__STANDALONE_DAG_PROCESSOR: "True"
    # How many DAG runs may be active for a single DAG at once
    AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: "4"
    AIRFLOW__CORE__TASK_RUNNER: "StandardTaskRunner"
    AIRFLOW__CORE__STORE_SERIALIZED_DAGS: "True"
    AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
  volumes:
    - ./jobs:/opt/airflow/jobs
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
  networks: [airflow]
  depends_on:
    airflow-postgres:
      condition: service_healthy

services:
  # -------------------- Spark --------------------
  spark-master:
    <<: *spark-common
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master"]
    ports:
      - "9090:8080"
      - "7077:7077"

  spark-worker:
    <<: *spark-common
    depends_on: [spark-master]
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077"]
    environment:
      - SPARK_MODE=worker
      # Increase defaults so a few concurrent Spark executors can be scheduled locally.
      # These are still overrideable via .env or environment variables.
      - SPARK_WORKER_CORES=${SPARK_WORKER_CORES:-6}
      - SPARK_WORKER_MEMORY=${SPARK_WORKER_MEMORY:-12g}
      - SPARK_MASTER_URL=spark://spark-master:7077
      - JAVA_HOME=/opt/java/openjdk

  # -------------------- Postgres --------------------
  airflow-postgres:
    image: postgres:14.0
    environment:
      - POSTGRES_USER=${AIRFLOW_PG_USER}
      - POSTGRES_PASSWORD=${AIRFLOW_PG_PASSWORD}
      - POSTGRES_DB=${AIRFLOW_PG_DB}
    networks: [airflow]
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB"]
      interval: 5s
      timeout: 3s
      retries: 20

  # -------------------- Sumo Postgres (dev) --------------------
  # This Postgres service is intended for local development/testing only.
  # It will initialize from any SQL files placed in ./db_init on first start.
  sumo-db:
    image: postgres:16
    restart: unless-stopped
    environment:
      - POSTGRES_USER=${DB_USERNAME}
      - POSTGRES_PASSWORD=${DB_PASSWORD}
      - POSTGRES_DB=${DB_NAME}
    volumes:
      - sumo-db-data:/var/lib/postgresql/data
    networks: [airflow]
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USERNAME} -d ${DB_NAME} -h localhost"]
      interval: 5s
      timeout: 3s
      retries: 20

  # -------------------- One-shot restore (runs after sumo-db is healthy)
  # Restores any .dump or .sql files found in ./db_init into the sumo-db
  sumo-db-restore:
    image: ${SUMO_RESTORE_IMAGE:-postgres:18}
    restart: "no"
    env_file:
      - .env
    environment:
      - DB_USERNAME=${DB_USERNAME}
      - DB_PASSWORD=${DB_PASSWORD}
      - DB_NAME=${DB_NAME}
      - DB_HOST=sumo-db
      - DB_PORT=5432
      - FORCE_RESTORE=1
    depends_on:
      sumo-db:
        condition: service_healthy
    volumes:
      - ./db_init:/db_init:ro
    entrypoint: ["/bin/bash", "/db_init/restore_sumo_db.sh"]
    networks: [airflow]

  

  # -------------------- One-shot DB init --------------------
  airflow-init:
    <<: *airflow-common
    restart: "no"
    command:
      - bash
      - -lc
      - |
        set -euo pipefail

        echo 'Running Airflow DB initialization...'
        airflow db init

        echo 'Creating initial user (idempotent)...'
        airflow users create \
          --username "$AIRFLOW_USERNAME" \
          --firstname "$AIRFLOW_FIRSTNAME" \
          --lastname "$AIRFLOW_LASTNAME" \
          --role Admin \
          --email "$AIRFLOW_EMAIL" \
          --password "$AIRFLOW_PASSWORD" || true

        echo 'Seeding spark_default connection...'
        cat > /tmp/spark_conn.json <<'JSON'
        {
          "conn_type": "spark",
          "host": "spark://spark-master:7077",
          "extra": {
            "deploy_mode": "cluster",
            "spark_binary": "spark-submit",
            "master": "spark://spark-master:7077"
          }
        }
        JSON

        airflow connections delete spark_default || true
        airflow connections add spark_default --conn-json "$(cat /tmp/spark_conn.json)"

        echo 'Seeding mongo_default connection (conn_type="mongo" + extras.srv to satisfy provider)...'
        airflow connections delete mongo_default || true
        if [ -n "${MONGO_URI:-}" ]; then
          echo "Building connection JSON for mongo_default from MONGO_URI"
          python3 -c 'import os,json; from urllib.parse import urlparse,unquote; uri=os.environ.get("MONGO_URI",""); parsed=urlparse(uri); username=unquote(parsed.username) if parsed.username else None; password=unquote(parsed.password) if parsed.password else None; host=parsed.hostname; schema=(parsed.path.lstrip("/") if parsed.path and parsed.path!="/" else os.environ.get("MONGO_DB_NAME")); conn={"conn_type":"mongo","host":host,"login":username,"password":password,"schema":schema,"extra":{"srv":"true"}}; import sys, json as __json; sys.stdout.write(__json.dumps(conn))' > /tmp/mongo_conn.json
          airflow connections add mongo_default --conn-json "$(cat /tmp/mongo_conn.json)" || true
        else
          echo "MONGO_URI not provided; skipping mongo_default seeding."
        fi

        airflow connections get mongo_default

        echo 'Init completed.'


  # -------------------- Webserver --------------------
  airflow-webserver:
    <<: *airflow-common
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    ports:
      - "8081:8080"
    command: >
      bash -lc '
        until airflow db check-migrations; do
          echo "Waiting for DB migrations to finish..."; sleep 3;
        done;
        exec airflow webserver
      '
    volumes:
      - ./jobs:/opt/airflow/jobs
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock

  # -------------------- Scheduler (Execution API INSIDE) --------------------
  airflow-scheduler:
    <<: *airflow-common
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment:
      <<: *airflow-env
    command: >
      bash -lc '
        set -euo pipefail
        until airflow db check-migrations; do
          echo "Waiting for DB migrations to finish..."; sleep 2;
        done;
        exec airflow scheduler
      '
    volumes:
      - ./jobs:/opt/airflow/jobs
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock

  # -------------------- DAG Processor --------------------
  airflow-dag-processor:
    <<: *airflow-common
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    command: >
      bash -lc '
        until airflow db check-migrations; do
          echo "Waiting for DB migrations to finish..."; sleep 3;
        done;
        exec airflow dag-processor
      '
    volumes:
      - ./jobs:/opt/airflow/jobs
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock

volumes:
  sumo-db-data: {}

networks:
  airflow:
