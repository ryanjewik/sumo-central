# docker-compose.yml

x-spark-common: &spark-common
  image: apache/spark:3.5.2-java17-python3
  volumes:
    - ./jobs:/opt/spark/work-dir/jobs
  networks: [airflow]
  environment:
    - SPARK_HOME=/opt/spark
    - JAVA_HOME=/opt/java/openjdk

x-airflow-common: &airflow-common
  build:
    context: .
    dockerfile: Dockerfile
  env_file:
    - .env
  environment: &airflow-env
    AIRFLOW__CORE__LOAD_EXAMPLES: "False"


    # DB connection from .env
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://${AIRFLOW_PG_USER}:${AIRFLOW_PG_PASSWORD}@airflow-postgres:5432/${AIRFLOW_PG_DB}"

    JAVA_HOME: /usr/lib/jvm/java-17-openjdk-amd64
    SPARK_HOME: /opt/spark

    # Executor & concurrency
    AIRFLOW__CORE__EXECUTOR: "LocalExecutor"
    AIRFLOW__CORE__PARALLELISM: "1"
    AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG: "1"
    AIRFLOW__SCHEDULER__STANDALONE_DAG_PROCESSOR: "True"
    AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: "1"
    AIRFLOW__CORE__TASK_RUNNER: "StandardTaskRunner"
    AIRFLOW__CORE__STORE_SERIALIZED_DAGS: "True"
    AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
  volumes:
    - ./jobs:/opt/airflow/jobs
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
  networks: [airflow]
  depends_on:
    airflow-postgres:
      condition: service_healthy

services:
  # -------------------- Spark --------------------
  spark-master:
    <<: *spark-common
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master"]
    ports:
      - "9090:8080"
      - "7077:7077"

  spark-worker:
    <<: *spark-common
    depends_on: [spark-master]
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077"]
    environment:
      - SPARK_MODE=worker
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_MASTER_URL=spark://spark-master:7077
      - JAVA_HOME=/opt/java/openjdk

  # -------------------- Postgres --------------------
  airflow-postgres:
    image: postgres:14.0
    environment:
      - POSTGRES_USER=${AIRFLOW_PG_USER}
      - POSTGRES_PASSWORD=${AIRFLOW_PG_PASSWORD}
      - POSTGRES_DB=${AIRFLOW_PG_DB}
    networks: [airflow]
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB"]
      interval: 5s
      timeout: 3s
      retries: 20

  # -------------------- One-shot DB init --------------------
  airflow-init:
    <<: *airflow-common
    restart: "no"
    command:
      - bash
      - -lc
      - |
        set -euo pipefail

        echo 'Running Airflow DB initialization...'
        airflow db init

        echo 'Creating initial user (idempotent)...'
        airflow users create \
          --username "$AIRFLOW_USERNAME" \
          --firstname "$AIRFLOW_FIRSTNAME" \
          --lastname "$AIRFLOW_LASTNAME" \
          --role Admin \
          --email "$AIRFLOW_EMAIL" \
          --password "$AIRFLOW_PASSWORD" || true

        echo 'Seeding spark_default connection...'
        cat > /tmp/spark_conn.json <<'JSON'
        {
          "conn_type": "spark",
          "host": "spark://spark-master:7077",
          "extra": {
            "deploy_mode": "cluster",
            "spark_binary": "spark-submit",
            "master": "spark://spark-master:7077"
          }
        }
        JSON

        airflow connections delete spark_default || true
        airflow connections add spark_default --conn-json "$(cat /tmp/spark_conn.json)"

        echo 'Init completed.'


  # -------------------- Webserver --------------------
  airflow-webserver:
    <<: *airflow-common
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    ports:
      - "8081:8080"
    command: >
      bash -lc '
        until airflow db check-migrations; do
          echo "Waiting for DB migrations to finish..."; sleep 3;
        done;
        exec airflow webserver
      '
    volumes:
      - ./jobs:/opt/airflow/jobs
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock

  # -------------------- Scheduler (Execution API INSIDE) --------------------
  airflow-scheduler:
    <<: *airflow-common
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment:
      <<: *airflow-env
    command: >
      bash -lc '
        set -euo pipefail
        until airflow db check-migrations; do
          echo "Waiting for DB migrations to finish..."; sleep 2;
        done;
        exec airflow scheduler
      '
    volumes:
      - ./jobs:/opt/airflow/jobs
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock

  # -------------------- DAG Processor --------------------
  airflow-dag-processor:
    <<: *airflow-common
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    command: >
      bash -lc '
        until airflow db check-migrations; do
          echo "Waiting for DB migrations to finish..."; sleep 3;
        done;
        exec airflow dag-processor
      '
    volumes:
      - ./jobs:/opt/airflow/jobs
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock

networks:
  airflow:
